#***************************************************************
# Where things are located
[Configurable]
train_files = colon/separated/list/of/files:supports/glob/*
parse_files = colon/separated/list/of/files:supports/glob/*

[Pretrained Vocab]
filename = location/of/pretrained/embeddings
# skips the first line of the file, which sometimes contains metadata about the embedding matrix
skip_header = True

#***************************************************************
# Embedding hyperparameters
[Char Vocab]
# {RNNEmbed, CNNEmbed, MLPEmbed}
embed_model = RNNEmbed

# The aggregated word vocab, pretrained vocab, and char vocab
[Multivocab]
# probability of dropping a word embedding
embed_keep_prob = .67

[Tag Vocab]
# probability of dropping a tag embedding
embed_keep_prob = .67

[RNN Embed]
# {RNNCell, GRUCell, LSTMCell, CifLSTMCell}
recur_cell = LSTMCell
# number of LSTM layers
n_layers = 1
# number of recurrent units
recur_size = 400
# probability of dropping a connection between timesteps at a single layer
recur_keep_prob = .67
# probability of dropping a connection between layers at a single timestep
ff_keep_prob = .67

#***************************************************************
# NLP model hyperparameters
[Tagger]
#if you only want it to produce the first column of tags, set this to just 'tags'
output_vocabs = tags:xtags
# {RNNCell, GRUCell, LSTMCell, CifLSTMCell}
recur_cell = LSTMCell
# number of LSTM layers
n_layers = 2
# number of recurrent units in each direction of the BiLSTM
recur_size = 400
# number of units in the tag classifier
mlp_size = 600
# probability of dropping a node in the MLP or the classifier
mlp_keep_prob = .67
# probability of dropping a connection between timesteps at a single layer
recur_keep_prob = .5
# probability of dropping a connection between layers at a single timestep
ff_keep_prob = .67

[Parser]
# if you only want it to use the first column of tags, set this to 'words:tags'
input_vocabs = words:tags:xtags
# {RNNCell, GRUCell, LSTMCell, CifLSTMCell}
recur_cell = LSTMCell
# number of layers
n_layers = 3
# number of recurrent units
recur_size = 400
# number of units in the edge classifier
arc_mlp_size = 600
# number of units in the label classifier (you probably want this to be small!)
rel_mlp_size = 100
# probability of dropping a node in the MLP or the classifier
mlp_keep_prob = .67
# probability of dropping a connection between timesteps at a single layer
recur_keep_prob = .67
# probability of dropping a connection between layers at a single timestep
ff_keep_prob = .67

#***************************************************************
# Training hyperparameters
[Network]
# {Parser, Tagger}
nlp_model = Parser
quit_after_n_iters_without_improvement = 5000
max_train_iters = 50000
